# 第一章-----python入门

## 1.1python是什么

Python是一个简单、易读、易记的编程语言，而且是开源的，可以免 费地自由使用

## 1.2python安装

> python3.x与2.x不兼容

### 1.2.2使用的外部库

Numpy库：数值计算库

Matplotlib库：画图的库，将实验结果可视化

## 1.3python解释器

在终端中输入 `python --version` 可以确定python版本

### 1.3.2数据类型

数据类型表示数据的性质，有整数（int）、小数（float）、字符串（str）等，可以使用type来查看

> 是注释 其后面的文字会被忽略

### 1.3.3变量

可以使用26个英文字母定义变量，同时可以给变量赋值，以及计算

 Python是属于“动态类型语言”的编程语言，所谓动态，是指变量的类 型是根据情况自动决定的。

### 1.3.4列表

可以汇总数据，类似于数组

索引是从0开始，python内部提供切片便于找到所找的数据

> -1代表最后一个元素
>
> -2代表倒数第二个元素

### 1.3.5字典

索引同样从0开始，每一个值都可以有一个key去寻找他

### 1.3.6布尔型

返回值只有false或者true，即0/1。

### 1.3.7if语句

`if...else` 当前面的条件（if hungry） 成立时，此处的代码会被执行

需要控制缩进tab

### 1.3.8for语句

for通常与in构成循环  `for... in ... :  语句结构`

## 1.4脚本文件

### 1.4.1保存为文件

创建的文件名后缀为py

> python  x.py 就可以执行此程序

### 1.4.2类

自己定义一个新的类型

``` py
class 类名：    

	def __init__(self, 参数, …): # 构造函数 只在生成类的实例时被调用一次      

		...    

	def 方法名1(self, 参数, …): # 方法1       

		...    

	def 方法名2(self, 参数, …): # 方法2
		...
```

## 1.5NumPy

作用：便捷数组和矩阵的计算

### 1.5.1导入NumPy

``` python
import numpy as np # 将numpy 作为np导入
```

### 1.5.2生成NumPy

```python
# 生成numpy数组需要使用np.array()
x = np.array([1.0,2.0,3.0])
--> [1. 2. 3.]

type(x)
--> <class 'numpy.ndarray'>
```

### 1.5.3Numpy的算数运算

两个数组之间运算：

- 元素个数相同--> 一一对应做相应的运算
- 元素个数不同--> 报错

数组对单一数值也可以进行运算：

- 分别进行运算

### 1.5.4Numpy的N维数组

可以生成多维数组并且可以进行运算，

一维数组--->向量，二维数组---> 矩阵，三维数组即以上--->张量/多维数组

可以将一般化之后的向量或矩阵等统 称为张量

> shape 可以查看矩阵形状
>
> dtype 可以查看矩阵元素的数据类型

其运算与数组运算一样，可以在相同形状下对应元素进行运算，也可以在通过标量对矩阵进行算术运算

### 1.5.5广播

在矩阵和标量相乘时，将标量扩展成与矩阵相同形状的一个矩阵(将标量变成二维数组进行运算)

因为NumPy有广播功能，所以不同形状的数组之间也可以进行运算。eg：

```python
x = np.array([[1, 2],[3, 4]])
y = np.array([10, 20])
--> array([[ 10, 40],
       		[ 30, 80]]) 
```

### 1.5.6访问元素

索引从0开始,与c语言一致

对各个元素访问

方法一：

```python
#通过索引找
x = np.array([[51, 55], [14, 19], [0, 4]])
print(x[0])
--> [51, 55]
```

方法二：

```python
# for循环
for i in x：
	print(i)
```

方法三：

```python
# numpy可以使用数组访问各个元素
X = x.flatten()   #将x转换为一维数组
```

## 1.6 Matplotlib

Matplotlib是用于绘制图形的库，使用Matplotlib可以绘制图形和实现数据的可视化

### 1.6.1绘制简单图形

```python
# sin曲线：
import numpy as np
import matplotlib.pyplot as plt

 # 生成数据
x = np.arange(0, 6, 0.1)   # 以0.1为单位，生成0到6的数据
y = np.sin(x)  #应用numpy的sin函数

# 绘制图形
 plt.plot(x, y)
 plt.show()
```

> arange 返回一个有终点和起点的固定步长的排列(等差数组)
>
> plot(x,y) 绘制二维图像
>
> show 将图形展示出来

### 1.6.2pyplot

```python
import numpy as np
import matplotlib.pyplot as plt

# 生成数据
x = np.arange(0, 6, 0.1) # 以0.1为单位，生成0到6的数据
y1 = np.sin(x)
y2 = np.cos(x)

# 绘制图形
plt.plot(x, y1, label="sin")
plt.plot(x, y2, linestyle = "--", label="cos") # 用虚线绘制
plt.xlabel("x") # 添加x轴标签
plt.ylabel("y") # 添加y轴标签
plt.title('sin & cos') # 标题
plt.legend()  #在图形上显示图例。由于之前为两条线分别设置了标签"sin"和"cos"，所以图例会显示这两条线的标签。
plt.show()
```

### 1.6.3显示图像

```py
import matplotlib.pyplot as plt
from matplotlib.image import imread

img = imread('lena.png') # 读入图像（设定合适的路径！）
plt.imshow(img)
plt.show()
```

imshow展示图像

## 1.7小结

- 认识数据类型，python基础的学习
- numpy库的导入和使用，array，arange，sin，cos
- matplotlib绘制图像，以及其内部函数的使用。plot，show，title，legend，image，show

# 第二章-----感知机

学习感知机的构造也就是学习通向神经网络和深度学习的一种重要思想

## 2.1感知机是什么

感知机接受输入多个信号，输出一个信号（类似于生物中的电信号）

**神经元激活**:传送过来的信号总和超过**某个限制(阈值)**才会输出1（0：不流，1：流）

多个输入信号都有各自固有的权重，权重越大重要性越高，与电阻(信号)电流(权重)的关系相似

## 2.2简单逻辑电路

### 2.2.1与门

与门：有两个输入和一个输出的门电路。当x1，x2均为1时，y为1

输入信号和输出信号的对应表称为**真值表**

### 2.2.2　与非门和或门

与非门：颠倒了与门的输出。当x1，x2均为1时，y为0

把实现与门的参数值的符号取反， 就可以实现与非门。

或门：只要有一个是1输出就是1

> 学习是确定合适的参数的过程，而人要做的是思考感知机的构造（模型），并把 训练数据交给计算机

2.2.3 总结

与门、与非门、或门的感知机构造是一样的，只有权重和阈值不同

## 2.3感知机的实现

### 2.3.1简单实现：

```python
def use(x1,x2):
    w1, w2, theta = 0.5, 0.5, 0.8
    t = x1*w1 + x2*w2
    if t <= t:
        return 0
    else:
        return 1
print(use(0, 0), use(1,1))
```

### 2.3.2导入权重和偏置

上述感知器实现流入均是大于theta，将theta = -b，则`x1*w1 + x2*w2 >= theta`----->`b + x1*w1 + x2*w2 >= 0`

而b则是偏置

> np.sum可以计算各个元素的总和

### 2.3.3使用权重和偏置的实现

偏置是调整神经元被激活的容易程度(输出信号为1的程度)的参数,是其激活的零界点

与门

```py
def AND(x1, x2):
    m = np.array([x1, x2])
    n = np.array([0.5, 0.5])
    b = -0.7
    t = np.sum(m*n) + b
    if t >= 0:
        return 1
    else:
        return 0
```

与非门

> 实现与门的参数值的符号取反， 就可以实现与非门。

```py
def NAND(x1, x2):
    m = np.array([x1, x2])
    n = np.array([-0.5, -0.5])
    b = 0.7
    t = np.sum(m*n) + b
    if t >= 0:
        return 1
    else:
        return 0
```

或门

> 仅权重和偏置不同

```py
def OR(x1, x2):
    m = np.array([x1, x2])
    n = np.array([0.5, 0.5])
    b = -0.2
    t = np.sum(m*n) + b
    if t <= 0:
        return 0
    else:
        return 1
```

## 2.4感知机的局限性

### 2.4.1异或门

异或门又称**逻辑异或**，仅当x1或x2一个为1才会输出1

发现：`d + x1*w1 + w2*x2 >=0`用线性知识无法实现，我们无法做出一条直线分割(0, 0)和(1, 1)

所以仅单层感知机无法表示异或门

### 2.4.2线性和非线性

**感知机的局限性**：只能由一条直线分割

非线性空间：曲线分割而成的空间

线性空间：直线分割而成的空间

## 2.5多层感知机

通过叠加层表示异或门

### 2.5.1已有门电路的组合

用已学的门电路实现异或门：

x1---------NAND------->s1 ---

​        |                                   |-------- AND -------->y

x2----------OR----------->s2 ---

### 2.5.2异或门的实现

```py
def XOR(x1, x2):
    s1 = NAND(x1, x2)
    s2 = OR(x1, x2)
    y = AND(s1, s2)
    return y
```

**多层感知机**：叠加了多层的感知机

多层感知机的实现：（2层感知机举例）

- 第0层的两个神经元接收输入信号，并将信号发送至第1层的神经元
- 第1层的神经元将信号发送至第2层的神经元，第2层的神经元输出y

通过叠加层使感知机进行更加灵活的表示

## 2.6从非与门到计算机

多层感知机可以表示计算机进行的处理，合理但是繁琐

### 2.7总结

感知机是什么：是具有输入和输出的算法，一种二元分类模型

与门：x1与x2均为1 ----> y = 1，其余y均为0

与非门：x1与x2均为1 ----->y = 0，其余y均为0

或门：只要有1----->y就为1

异或门：当且仅当其中一个为1------>y才为1                2层感知机

# 第三章神经网络

解决设定合适且符合预期的输入与输出的权重

## 3.1从感知机到神经网络

### 3.1.2复习感知机

b--->偏置，用于控制神经元被激活的容易程度；

w1和w2 ---->各个信号的权重，用于控制各个信号的重要性。

### 3.1.3激活函数登场

激活函数：将输入信号的总和转换为输出信号，y =h(b+w1x1+w2x2) ，h(x) 将x转化为输出y，是一种非线性转化，

## 3.2激活函数

将激活函数从阶跃函数转换成其他函数，

### 3.2.1sigmoid函数

神经网络中的一个激活函数：h(x)=1/(1+exp(-x))    epx(-x)= e的-x次方

### 3.2.2阶跃函数的实现

```python
def function(x):
    if x>0:
        return 1
    else:
        return 0
```

### 3.2.5sigmoid和阶跃函数的比较

- 不同点：
  - 平滑程度
    - sigmoid函数是一条平滑的去边
    - 阶跃函数变化剧烈
  - 返回值
    - sigmoid可以返回实数
    - 阶跃函数返回0或1

> sigmoid输入多少调出多少
>
> 阶跃函数只能返回0或者1

- 相同点
  - 当输入不重要的信息时都输出较小的值
  - 不管信号大小，输出都在0-1之间
  - 均为非线性函数

### 3.2.6非线性函数

为了发挥叠加层所带来的又是，**激活函数必须使用非线性函数**

### 3.2.7ReLU函数

输入大于0时直接输出该值；输入小于0时输出0

## 3.3多维数组的运算

### 3.3.1多维数组

> dim获取数组维数
>
> shape获取数组形状     --->返回tuple                                                                                                                                                                                                                                                                      

### 3.3.2矩阵乘法

> dot矩阵相乘，点积

矩阵相乘第一个矩阵的第一行元素的个数和第二个矩阵的第二列的个数一致即可
$$
\begin{bmatrix}
    \color{#FF0}{3} & 2 \\
    \color{#FF0}{2} & 3 \\
    \color{#FF0}{5} & 4 \\
   \end{bmatrix}
*\left[
 \begin{matrix}
   \color{#FF0}{1} & \color{#FF0}{3} & \color{#FF0}{2}\\
   2 & 1 & 3\\
  \end{matrix}
  \right]
$$
3、2、5--->三位数    1、3、2--->三位数    才能相乘

### 3.3.3神经网络的内积

矩阵相乘便于计算

### 3.4 3层神经网络的实现

### 3.4.1符号确认

$$
a^1_1 ->上面的1表示层数，即第几层神经元\\
底下的1表示输入层\\
w^1_{1\space1}->上面的1表示第n层的权重\\
底下第一个1表示输出层第1个元素\\
底下第二个1表示前一层的第1个元素
$$



### 3.4.2各层之间信号传递的实现

$$
a^1_1 = 
w^1_{1\space \space1}x1+
w^1_{2 \space \space2}x2+
b^1_1
$$

## 3.5输出层的设计

### 3.5.1   恒等函数和softmax函数

恒等函数将输入按照原样输出对于输入的信息不加改动的直接输出。
$$
y_k=\frac{\exp(a_k)}{\sum\limits_{i=1}^n\exp(a_i)}
$$

$$
\exp(x) = e^x\\
a_k代表输入信号，是指数函数\\
分子是指数函数求和
$$

由于e的x次方数值可能较大超出计算机处理范围就会溢出

所以式子可以写成
$$
y_k=\frac{\exp(a_k)}{\sum\limits_{i=1}^n\exp(a_i)}\\=
\frac{C*\exp(a_k)}{C*\sum\limits_{i=1}^n\exp(a_i)}\\=
\frac{\exp(a_k+\log{C})}{\sum\limits_{i=1}^n\exp(a_i+\log{C})}\\=
\frac{\exp(a_k)+C_1}{\sum\limits_{i=1}^n\exp(a_i)+C_1}
$$

可以通过矩阵减去最大值减去其最大值 来计算

### 3.5.3  sofftmax函数的特征

输出在0.0-1.0之间，**softmax函数的输出值的总和是1**，其输出解释为“概率”---->概率和为1

使用softmax函数，输出值最大的神经元的位置也不会变。softmaxx函数可以省略，

> **求解机器学习问题的步骤可以分为"学习"，"推理"**，
>
> 学习阶段进行模型的学习（在模型中训练调整参数）；推理阶段用学到的模型比对未知数据进行推理，此阶段一般会忽略输出层的softmax函数。                                                                                                                                                                                    

### 3.5.4输出层的神经元数量

输出层的神经元数量是根据解决的问题来决定，输出层的神经元数量一般设定为类别的数量。

## 3.6手写数字识别

神经网络的推理处理------>向前传播

### 3.6.1MNIST数据集

MNIST机器学习领域的数据集，被用于从简单的实验到发表论文研究

> normalize 是否将输入图像正规化为0.0·1.0(将图像的各个像素值除以255)，false:输入图像的像素会保持原来的0·255
>
> flatten设置图像true输入图像会保存为由784个元素构成的以为数组false输入1 * 28 * 28
>
> one_hot_label仅正确解释标签为1，其余皆为0的数组，one_hot_label为false只是想7、2这样简单保存正确解标签；当one_hot_label为true仅保存为one-hot

### 3.6.2神经网络的推理处理

> predict函数进行分类
>
> argmax函数取出数组最大索引

**正规化**：把数据限定在某个范围内处理

**预处理**：对神经网络的输入数据进行某种既定的转换

### 3.6.3批处理

将n张图片作为数据，结果*n，打包方式的输入数据成为批                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            

批处理可以缩短处理时间

> argmax 函数，将二维数组按行平铺成一维数组输出第一个最大值的索引
>
> 参数axis为1时按照行分别找出每行的最大值的索引
>
> 为0时按照列找出最大值的索引

## 3.7总结

- 神经网络中的激活函数使用平滑变化的sigmoid函数或ReLU函数
- 通过巧妙地使用NumPy多维数组，可以高效地实现神经网络
- 机器学习的问题大体上可以分为回归问题和分类问题
- 关于输出层的激活函数，回归问题中一般用恒等函数，分类问题中 一般用softmax函数
- 分类问题中，输出层的神经元的数量设置为要分类的类别数
- 输入数据的集合称为批。通过以批为单位进行推理处理，能够实现 高速的运算

# 第四章神经网络学习

学习的目的就是以该损失函数为基准，找出能使它 的值达到最小的权重参数

函数斜率的梯度法：找出尽可能小的损失函数的值

## 4.1从数据中学习

神经网络的特征：从数据中学习---> 可以**由数据自动决定**权重参数的值

### 4.1.1数据驱动

数据是机器学习的核心

通过有效利用数据解决问题

- 从图像中提取特征量【可以从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器】，再用机器学习技术学习这些特征量的模式。

> 特征量一般用向量表示
>
> 常用的特征量包括SIFT、SURF、HOG，将图像数据转化为向量，对转换后的向量使用SVM、KNN等分类器进行学习

深度学习称为端到端机器学习

**端到端**是指从一端到另一端的意思，也就是 从原始数据（输入）中获得目标结果（输出）的意思。

神经网络的优点：对所有问题都可以用同样的流程来解决同样的流程来解决

### 4.1.2训练数据和测试数据

数据分为训练数据（也称作监督数据）和测试数据

原因：对模型的泛化能力的追求 

目的：为了正确评价模型的泛化能 力

训练数据进行学习(寻找最优的参数)------->测试数据评价训练（得到的模型的实际能力）

泛化能力是指处理未被观察过的数据（不包含在训练数据中的数据）的 能力

仅仅用一个数据集去学习和评价参数，是无法进行正确评价的。 这样会导致可以顺利地处理某个数据集，但无法处理其他数据集的情况。

过拟合：只对某个数据集过度拟合的状态

##  4.2 损失函数

神经网络的学习中 所用的指标称为损失函数

这个损失函数可以使用任意函数， 但一般用均方误差和交叉熵误差等

### 4.2.1均方误差

对于每个样本，都需要计算所有类别的预测概率和标记值差的平方和，再求平均
$$
E=\frac{1}{2}\sum_k(y_k-t_k)^2 \\
y_k是表示神经网络的输出，t_k表示监督数据，k表示数据的维数
$$

### 4.2.2交叉熵误差

$$
E=-\sum_kt_k\ln{y_k}
$$

### 4.2.3mini-batch学习

$$
E=-\frac{1}{N}\sum_k\sum_nt_{kn}\ln{y_{kn}}
$$

**mini-batch学习**：神经网络的学习是从训练数据中选出一批数据（称为mini-batch,小批量）然后对每个mini-batch进行学习

###  4.2.4　mini-batch版交叉熵误差的实现

可以同时处理单 个数据和批量数据（数据作为batch集中输入）两种情况的函数

```python
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size
```

batch_size  = 5 生成  [ y[0,2], y[1,7], y[2,0],  y[3,9], y[4,4]   ]

### 4.2.5　为何要设定损失函数

在进行神经网络的学习时，不能将识别精度作为指标。因为如果以 识别精度为指标，则参数的导数在绝大多数地方都会变为0。

识别精度对微小的参数变化基本上没有什么反应

## 4.3 数值微分

梯度法使用梯度的信息决定前进的方向



> ndim数组维度 
>
> reshape(a,b)生成a行b列

### 4.3.1　导数

以x为中心，计 算它左右两边的差分，所以也称为**中心差分**

而(x+h)和x之间的差分称为 **前向差分**

### 4.3.2　数值微分的例子

```python
import numpy as np
import matplotlib.pylab as plt


def function_1(x):
    return 0.01 * x ** 2 + 0.1 * x


def numerical_diff(f, x):
    h = 1e-4  # 0.0001
    return (f(x + h) - f(x - h)) / (2 * h)


x = np.arange(0.0, 20.0, 0.1)  # 以0.1为单位，从0到20的数组x
y = function_1(x)
plt.xlabel("x")
plt.ylabel("f(x)")
plt.plot(x, y)
plt.show()
print(numerical_diff(function_1, 5))
```





### 4.3.3　偏导数

**偏导数**::有多个变量的函数的导数

## 4.4 梯度

**梯度**：由全部变量的偏导数汇总 而成的向量

负梯度方向是梯度法中变量的更新方向

梯度会指向各点处的函数值降低的方向，**梯度指示的方向 是各点处的函数值减小最多的方向**

### **4.4.1　梯度法**

梯度表示的是各点处的函数值**减小最多(并最小值)**的方向

寻找最小值的梯度法称为**梯度下降法**

寻找最大值的梯度法称为**梯度上升法**
$$
x_0=x_0-\eta\frac{\partial f}{\partial x_0}\\
x_01=x_1-\eta\frac{\partial f}{\partial x_1}\\

η表示更新量
$$


在神经网络的学习中，称为**学习率**

在神经网络的学习中，一般会 一边改变学习率的值，一边确认学习是否正确进行了

学习率过大的话，会发散成一个很大的值,学习率过小的话，基本上没怎么更新就结束了

### 4.4.2　神经网络的梯度

神经网络的学习也要求梯度(损失函数关于权重参数的梯度)

## 4.5 学习算法的实现

mini-batch----->计算梯度----->更新参数---->重复

由于mini batch数是随机的，所以又称随机梯度下降法

### 4.5.1　2层神经网络的类

>  params保存神经网络的参数的字典型变量
>
> grads保存梯度的字典型变量（numerical_gradient()方法的返回值）

权重使用符合高斯 分布的随机数进行初始化，偏置使用0进行初始化

### 4.5.2　mini-batch的实现

神经网络的学习的实现使用的是前面介绍过的mini-batch学习

### 4.5.3　基于测试数据的评价

**epoch**是一个单位，一个                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     表示学习中所有训练数据均被使用过一次时的更新次数

## 4.6 小结

- 机器学习中使用的数据集分为训练数据和测试数据。  
- 神经网络用训练数据进行学习，并用测试数据评价学习到的模型的 泛化能力。   
- 神经网络的学习以损失函数为指标，更新权重参数，以使损失函数 的值减小。  
- 利用某个给定的微小值的差分求导数的过程，称为数值微分。 
- 利用数值微分，可以计算权重参数的梯度。 
- 数值微分虽然费时间，但是实现起来很简单。

# 第五章误差反向传播

两种方法了解误差反向传播

-   数学式
-   计算图

## 5.1计算图

计算图将计算过程用图形（数据结构图多个节点和边表示）表示

流水线工作

### 5.1.1用计算图求解

流程

1.  构建计算图
2.  在计算图上，从右往左计算

**正向传播**：从左往右，计算图的出发点到结束点的传播

**反向传播**：从右往左

### 5.1.2局部计算

计算图的特征：通过传递局部计算获得最终结果

局部计算：无论全局发生了什么都只能根据与自己相关的信息输出接下来的结果

避免全局计算的复杂

### 5.1.3计算图的优点

-   局部计算，通过不断细化全局复杂运算，分解成多个局部计算，从而简化计算
-   可以保留中间值
-   可以用过正向、反向传播高效计算导数

## 5.2链式法则

### 5.2.1计算图的反向传播

计算顺序：信号乘节点的局部倒数

### 5.2.2什么是链式法则

复合函数:多个函数构成

如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示。

### 5.2.3链式法则和计算图

对每个信号进行偏导计算

## 5.3反向传播

### 5.3.1加法节点的反向传播

由于加法式子导数均为1，所以反向传播过程会传相同的值到下一个节点

### 5.3.2乘法反向传播

将上游的值乘以正向传播时的输入信号的翻转值后转递给下游

以`z = x * y`为例子：正向传播信号为x反向传播则为y

​									正向传播信号是y反向传播则为x

## 5.4简单层的实现

计算图的乘法节点称为‘乘法层’，加法节点称为‘加法层’。

### 5.4.1乘法层的实现

层的实现有两个共通的方法(接口)forward()和backward。forward对应正向传播，backward对应反向传播



## 5.5激活函数层的实现

### 5.5.1RELU层

y = x函数（x = 0）

反向传播

### 5.5.2sigmoid函数

省略反向传播过程效率更加高级

## 5.6Affine和softmax层的实现

### 5.6.1Affine

Affine：神经网络的正向传播中进行的矩阵的乘积运算的处理
$$
\frac{\partial L}{\partial X}=
\frac{\partial L}{\partial Y}  \cdot
W^T\\
\frac{\partial L}{\partial W} = 
x^T\cdot   \frac{\partial L}{\partial Y}
$$

### 5.6.2批版本的Affine层

N个数据一起正向传播

### 5.6.3softmax-with-Loss层

softmax推理过程不使用，学习阶段则需要使用

softmax函数包含作为损失函数的交叉熵误差，称作softmax-with-loss

**神经网络的反向传播**会把这个差分表示的误差传递给 前面的层，这是神经网络学习中的重要性质

## 5.7误差反向传播法的实现

### 5.7.1神经网络学习的全貌图

神经网络中有合适的权重和偏置，调整权重和偏置以便拟合训练数据的 过程称为学习。神经网络的学习分为下面4个步骤，如下

1.  从训练数据中随机选择一部分数据，mimi-batch
2.  计算梯度：计算损失函数关于各个权重参数的梯度
3.  更新参数将权重参数沿梯度方向进行微小的更新
4.  重复：重复步骤1、步骤2、步骤3

### 5.7.3误差反向传播法的梯度确认

**梯度确认**确认数值微分求出的梯度结果和误差反向传播法求出的结果是否一致（严格地讲，是非常相近）的操作

## 5.8小结

# 第六章与学习相关的技巧

最优化：找到并解决最优参数的问题

### 6.1.2SGD

SGD: 
$$
w<-\space\space  w-\eta\frac{\partial L}{\partial W}
$$

### 6.1.3SGD的缺点

由于梯度指向的是减小值的最小值，而非函数最小值的方向，就会非常低效

### 6.1.4Momentum

Momentum是动量的意思
$$
v<-\quad \alpha v - \eta \frac{\partial L}{\partial M}\\
w <- w +v
$$




# 补充

## 1.交叉熵

### 1.1熵

#### 1.1.1定义

混乱程度，不确定性

定义：无损编码事件信息的最小平均编码长度

交叉熵:“交叉”指的是真实分布和估计的交叉，并且希望这个值越小越好，所以可以用来做损失函数
