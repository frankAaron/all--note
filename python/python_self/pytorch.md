# 第一章深度学习介绍

## 1.1人工智能

1.  弱人工智能：擅长单个领域
2.  强人工智能：类似于人类
3.  超人工智能：强于人类

## 1.2数据挖掘、机器学习与深度学习

### 1.2.1数据挖掘

数据挖掘就是在数据库中发现有用的信息并加以分析

从大量数据中分析得到数据之间的特征

### 1.2.2机器学习

1.  监督学习：从训练集中学习一个函数，可以用这个函数预测新数据，常见的监督学习算法包括回归与分类
2.  无监督学习：训练集没有人为标注的结果。
3.  半监督学习：一种介于监督与无监督之间的学习方法
4.  迁移学习：已经训练好的模型参数迁移到新的模型训练数据集
5.  增强学习：通过观察周围环境来学习。每个动作都会对环境有所影响，学习对象根据观察到的周围环境的反馈来判断

### 1.2.3深度学习

机器学习的分支通过复杂的结构自动提取数据特征。深度学习

# 第二章深度学习框架

## 2.1深度学习框架介绍

1.  Tensorflow：谷歌开源
2.  Caffe
3.  Theano
4.  torch
    -   Lua编写
5.  MXNet
    -   优点：占用内存低，性能好
    -   缺点：教程不完善，社区小

## 2.2PyTorch介绍

### 2.2.1什么是pytorch

是一个优先深度学习的框架，支持动态神经网络

### 2.2.2为什么要使用pytorch

1.  反向自动求导，零延迟
2.  命令式体验
# 第三章多层全连接神经网络

## 3.1pytorch基础
### 3.1.1 张量Tensor
一维张量---> 向量（无坐标轴）
二维张量---> 矩阵

### 3.1.2 变量Variable
可以自动求导，变量可以放到计算图中进行传播

与tensor的区别就是自主求导

### 3.1.3 数据集Dataset

在处理任何机器学习问题之前都需要数据读取，并进行预处理

### 3.1.4 模组nn.Moudle

在pytorch中编写神经网络，所有的层结构和损失函数都来自torch.nn所有的模型构建都是从这个基类nn.Module继承的

### 3.1.5优化torch.optim

在机器学习或者深度学习中，我们需要通过修改参数使得损失函数最小化（或者最大话），优化算法就是一种调整模型参数更新的策略

**一阶优化算法**

梯度下降：通过寻找最小值，控制方差，更新模型参数，最终使模型收敛

**二阶优化算法**

二阶优化算法使用了二阶导数来最小化或者最大化损失函数，主要基于牛顿法，但是成本高

# 补充

## 1.牛顿法

### 1.1一元函数

泰勒展开式(忽略二次以上的项)
$$
f(x)=f(x_0)+\dot{f(x_0)}(x-x_0)+\frac{1}{2}+\dot{f(x_0)(x-x_0)^2}
$$
寻找导数为0的点（对两边同时求导）
$$
\dot{f(x)}=\dot{f(x_0)}+\ddot{f(x_0)(x-x_0)}=0
$$
解得
$$
x=x_0-\frac{\dot{f(x_0)}}{\ddot{f(x_0)}}
$$
从x0走到下，接下来重复这个过程，直达到达导数为0的点，由此得到牛顿法的迭代公式：
$$
x_{t+1}=x_t-\frac{\dot{f(x_t)}}{\ddot{f(x_t)}}
$$
